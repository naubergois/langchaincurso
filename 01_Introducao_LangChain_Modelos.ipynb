{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Introdução ao LangChain e Modelos\n",
    "\n",
    "Bem-vindo ao curso prático de LangChain! Neste primeiro notebook, vamos configurar nosso ambiente e fazer nossas primeiras chamadas a modelos de linguagem (LLMs) e ChatModels.\n",
    "\n",
    "**Objetivos:**\n",
    "- Instalar as bibliotecas necessárias.\n",
    "- Configurar chaves de API.\n",
    "- Diferenciar LLMs de ChatModels.\n",
    "- Fazer a primeira chamada (\"Hello World\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48521a8",
   "metadata": {},
   "source": [
    "# Explicação Detalhada do Assunto\n",
    "\n",
    "# 01. Introdução ao LangChain e Modelos\n",
    "\n",
    "Bem-vindo ao fascinante mundo do LangChain! Este notebook marca o início da sua jornada prática neste poderoso framework, projetado para simplificar e potencializar o desenvolvimento de aplicações baseadas em Inteligência Artificial Generativa. Prepare-se para dar os primeiros passos na construção de soluções inteligentes e inovadoras!\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Neste primeiro notebook, focaremos na configuração do seu ambiente de desenvolvimento e na realização das suas primeiras interações com modelos de linguagem (LLMs) utilizando o LangChain. Aprenderemos a instalar as bibliotecas necessárias, configurar a chave de API para acessar os modelos da OpenAI e explorar as diferentes formas de interagir com os modelos, desde prompts simples até a utilização de mensagens estruturadas.\n",
    "\n",
    "## Conceitos Chave\n",
    "\n",
    "Para uma melhor compreensão, vamos definir alguns conceitos-chave que serão abordados ao longo do curso:\n",
    "\n",
    "*   **LLMs (Large Language Models):** Modelos de linguagem de grande escala, como os da OpenAI, capazes de gerar texto, traduzir idiomas, responder a perguntas e muito mais.\n",
    "*   **ChatModels:** Uma abstração do LangChain para modelos de linguagem projetados especificamente para conversação. Diferente dos LLMs tradicionais, os ChatModels são otimizados para lidar com múltiplos turnos de diálogo e manter o contexto da conversa.\n",
    "*   **Prompts:** As instruções ou perguntas que fornecemos aos modelos de linguagem para obter uma resposta. A qualidade do prompt é crucial para obter os resultados desejados.\n",
    "*   **Chains:** Sequências de chamadas a LLMs ou outras utilidades. Permitem criar fluxos de trabalho complexos e automatizar tarefas. (Será abordado em notebooks futuros)\n",
    "*   **RAG (Retrieval Augmented Generation):** Uma técnica que combina a capacidade de geração de texto dos LLMs com a capacidade de buscar informações relevantes em uma base de conhecimento externa. (Será abordado em notebooks futuros)\n",
    "*   **Memória:** A capacidade de um modelo de linguagem de \"lembrar\" informações de interações passadas. Permite criar conversas mais contextuais e personalizadas. (Será abordado em notebooks futuros)\n",
    "\n",
    "## Objetivos de Aprendizado\n",
    "\n",
    "Ao concluir este notebook, você será capaz de:\n",
    "\n",
    "1.  Instalar o LangChain e suas dependências.\n",
    "2.  Configurar a autenticação para acessar os modelos da OpenAI.\n",
    "3.  Inicializar um `ChatModel` do LangChain.\n",
    "4.  Enviar prompts simples e estruturados aos modelos de linguagem.\n",
    "5.  Compreender a diferença entre `LLMs` e `ChatModels` e quando usar cada um.\n",
    "6.  Utilizar diferentes tipos de mensagens (SystemMessage, HumanMessage, AIMessage) para controlar o comportamento do modelo.\n",
    "\n",
    "## Importância no Ecossistema LangChain\n",
    "\n",
    "Este notebook é fundamental porque estabelece as bases para todo o seu aprendizado no LangChain. Dominar a instalação, configuração e interação básica com os modelos de linguagem é essencial para construir aplicações mais complexas e aproveitar ao máximo o potencial do framework. Sem esses fundamentos, será difícil progredir para tópicos mais avançados, como Chains, RAG e Memória.\n",
    "\n",
    "Então, prepare-se para mergulhar no mundo do LangChain e desbloquear o poder da IA Generativa! Vamos começar!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação\n",
    "\n",
    "Vamos instalar o `langchain`, `langchain-openai` e outras dependências comuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:16:07.945953Z",
     "iopub.status.busy": "2026-02-03T11:16:07.945867Z",
     "iopub.status.idle": "2026-02-03T11:16:09.904171Z",
     "shell.execute_reply": "2026-02-03T11:16:09.902523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INJECTION START ###\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Carrega .env do local ou de pastas comuns\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Autenticação automática do script\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Look for .env in scripts folder\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# !pip install -qU langchain langchain-openai langchain-community python-dotenv # Script-patched: assumed installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuração da API Key\n",
    "\n",
    "Para usar os modelos da OpenAI, precisamos de uma API Key. No Colab, a forma mais segura é usar o `google.colab.userdata` se você tiver salvado a chave nos segredos do Colab, ou usar `getpass` para digitar na hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:16:09.930317Z",
     "iopub.status.busy": "2026-02-03T11:16:09.930080Z",
     "iopub.status.idle": "2026-02-03T11:16:09.932518Z",
     "shell.execute_reply": "2026-02-03T11:16:09.932268Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    pass # Script-patched\n",
    "except:\n",
    "    pass # Added to avoid IndentationError After patching\n",
    "    # Caso não esteja nos segredos do Colab, pede input\n",
    "#     pass # Script-patched # Script-patched: using env var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ChatModels vs LLMs\n",
    "\n",
    "No LangChain, existem duas abstrações principais para modelos:\n",
    "1. **LLMs**: Recebem uma string e retornam uma string (modelos de completude de texto mais antigos).\n",
    "2. **ChatModels**: Recebem uma lista de mensagens (System, Human, AI) e retornam uma mensagem (modelos de chat modernos, como GPT-3.5 e GPT-4).\n",
    "\n",
    "Vamos focar em **ChatModels**, pois são o padrão atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:16:09.933807Z",
     "iopub.status.busy": "2026-02-03T11:16:09.933719Z",
     "iopub.status.idle": "2026-02-03T11:16:13.487449Z",
     "shell.execute_reply": "2026-02-03T11:16:13.486719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.6). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:47: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google.generativeai.caching import CachedContent  # type: ignore[import]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x111ba6610>, default_metadata=())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Inicializando o modelo\n",
    "# temperature=0 deixa o modelo mais determinístico\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Invocando o Modelo\n",
    "\n",
    "A forma mais simples de usar é chamar o método `.invoke()`. Podemos passar uma string simples, que será convertida automaticamente para uma mensagem de usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:16:13.493698Z",
     "iopub.status.busy": "2026-02-03T11:16:13.493221Z",
     "iopub.status.idle": "2026-02-03T11:16:15.237526Z",
     "shell.execute_reply": "2026-02-03T11:16:15.236564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O primeiro homem a pisar na Lua foi o astronauta americano **Neil Armstrong**, em 20 de julho de 1969.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Quem foi o primeiro homem a pisar na lua?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tipos de Mensagens\n",
    "\n",
    "Para ter mais controle, usamos tipos de mensagens:\n",
    "- `SystemMessage`: Define o comportamento ou persona do sistema.\n",
    "- `HumanMessage`: A entrada do usuário.\n",
    "- `AIMessage`: A resposta do modelo (usada em históricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:16:15.245968Z",
     "iopub.status.busy": "2026-02-03T11:16:15.245367Z",
     "iopub.status.idle": "2026-02-03T11:16:16.263904Z",
     "shell.execute_reply": "2026-02-03T11:16:16.263575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, deixe-me pensar... Isso é um pouco complicado. Hum... 2 + 2... É 4. Eu sei, é chocante, certo?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente sarcástico e engraçado.\"),\n",
    "    HumanMessage(content=\"Quanto é 2 + 2?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, aprendemos:\n",
    "1. Instalar o LangChain.\n",
    "2. Autenticar com a OpenAI.\n",
    "3. Inicializar um `ChatOpenAI`.\n",
    "4. Enviar prompts simples e estruturados com mensagens.\n",
    "\n",
    "No próximo notebook, veremos como usar **Prompt Templates** para tornar nossos inputs dinâmicos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
