{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Introdução ao LangGraph: Fluxos Cíclicos\n",
    "\n",
    "Até agora, nossas Chains eram lineares (DAGs). Mas e se precisarmos de um **loop**? Ex: \"Escreva um código, teste. Se der erro, corrija e teste de novo\". Para isso serve o `LangGraph`.\n",
    "\n",
    "**Objetivos:**\n",
    "- Entender `StateGraph`, `Nodes` e `Edges`.\n",
    "- Criar um fluxo com condicional (Router)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff660d16",
   "metadata": {},
   "source": [
    "# Explicação Detalhada do Assunto\n",
    "\n",
    "# 23. Introdução ao LangGraph: Fluxos Cíclicos\n",
    "\n",
    "Este notebook marca uma transição crucial no nosso aprendizado de LangChain, nos levando além das Chains lineares (DAGs - Directed Acyclic Graphs) para explorar fluxos de trabalho mais dinâmicos e complexos: os **fluxos cíclicos**. Prepare-se para construir aplicações de IA que podem aprender, iterar e refinar suas respostas com base em feedback contínuo.\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Neste notebook, vamos mergulhar no LangGraph e aprender como construir grafos de execução que permitem loops, ou seja, fluxos de trabalho que podem retornar a nós anteriores para refinamento.  Vamos construir um exemplo prático onde um agente gera uma resposta, um crítico a avalia e, se necessário, o agente refina a resposta em um ciclo iterativo até atingir um nível de qualidade aceitável.\n",
    "\n",
    "## Conceitos Chave\n",
    "\n",
    "Para entender completamente este notebook, é importante estar familiarizado com os seguintes conceitos:\n",
    "\n",
    "*   **Chains (Cadeias)**: Sequências de chamadas a LLMs (Large Language Models) ou outras utilidades, executadas em uma ordem específica. Até agora, nossas Chains eram lineares, com um fluxo unidirecional.\n",
    "*   **DAG (Directed Acyclic Graph)**: Um grafo direcionado sem ciclos. As Chains lineares que vimos até agora são DAGs.\n",
    "*   **Fluxos Cíclicos**: Fluxos de trabalho que permitem loops, ou seja, a capacidade de retornar a nós anteriores para refinamento ou iteração. Isso é fundamental para construir aplicações que podem aprender e melhorar ao longo do tempo.\n",
    "*   **Estado (State)**: Um dicionário (ou objeto Pydantic) compartilhado entre todos os nós do grafo. Ele armazena informações relevantes para o fluxo de trabalho, como a pergunta original, a resposta gerada e o número de revisões.\n",
    "*   **Nós (Nodes)**: Funções Python simples que recebem o estado e retornam uma atualização para ele. Cada nó representa uma etapa no fluxo de trabalho, como gerar uma resposta ou avaliar sua qualidade.\n",
    "*   **Arestas (Edges)**: Conexões entre os nós do grafo. Elas definem o fluxo de execução e podem ser condicionais, permitindo que o fluxo de trabalho se adapte com base no estado atual.\n",
    "\n",
    "## Objetivos de Aprendizado\n",
    "\n",
    "Ao concluir este notebook, você será capaz de:\n",
    "\n",
    "*   Compreender o conceito de fluxos cíclicos e sua importância na construção de aplicações de IA mais inteligentes.\n",
    "*   Definir o estado compartilhado entre os nós do grafo.\n",
    "*   Criar nós que executam tarefas específicas e atualizam o estado.\n",
    "*   Implementar lógica condicional para controlar o fluxo de execução do grafo.\n",
    "*   Montar um grafo LangGraph com nós e arestas, definindo um fluxo de trabalho cíclico.\n",
    "*   Executar o grafo e observar como ele itera para refinar a resposta.\n",
    "\n",
    "## Importância no Ecossistema LangChain\n",
    "\n",
    "A capacidade de criar fluxos cíclicos é um passo fundamental para construir aplicações de IA generativa mais avançadas e adaptativas.  LangGraph fornece as ferramentas necessárias para orquestrar esses fluxos complexos, permitindo que você crie agentes que podem aprender com seus erros, refinar suas respostas e se adaptar a novas situações.  Dominar LangGraph abre um mundo de possibilidades para construir aplicações de IA mais poderosas e eficazes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:05.242556Z",
     "iopub.status.busy": "2026-02-02T02:36:05.242295Z",
     "iopub.status.idle": "2026-02-02T02:36:05.273107Z",
     "shell.execute_reply": "2026-02-02T02:36:05.271767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INJECTION START ###\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['OPENAI_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# !pip install -qU langchain langchain-openai langchain-community langgraph # Script-patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:05.294379Z",
     "iopub.status.busy": "2026-02-02T02:36:05.293105Z",
     "iopub.status.idle": "2026-02-02T02:36:05.306287Z",
     "shell.execute_reply": "2026-02-02T02:36:05.304605Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "except:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definindo o Estado (State)\n",
    "\n",
    "O Estado é um dicionário (ou objeto Pydantic) compartilhado entre todos os nós do grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:05.309588Z",
     "iopub.status.busy": "2026-02-02T02:36:05.309335Z",
     "iopub.status.idle": "2026-02-02T02:36:05.314656Z",
     "shell.execute_reply": "2026-02-02T02:36:05.314046Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class AgenteState(TypedDict):\n",
    "    pergunta: str\n",
    "    resposta: str\n",
    "    revisoes: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definindo os Nós (Nodes)\n",
    "\n",
    "São funções python simples que recebem o estado e retornam uma atualização para ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:05.316905Z",
     "iopub.status.busy": "2026-02-02T02:36:05.316679Z",
     "iopub.status.idle": "2026-02-02T02:36:06.800216Z",
     "shell.execute_reply": "2026-02-02T02:36:06.799359Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.6). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:47: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google.generativeai.caching import CachedContent  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "def gerador(state: AgenteState):\n",
    "    print(f\"--- GERANDO (Tentativa {state['revisoes']}) ---\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"Responda a pergunta de forma breve: {pergunta}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    res = chain.invoke({\"pergunta\": state['pergunta']})\n",
    "    return {\"resposta\": res, \"revisoes\": state['revisoes'] + 1}\n",
    "\n",
    "def critico(state: AgenteState):\n",
    "    print(\"--- CRITICANDO ---\")\n",
    "    # Simulação: se a resposta tiver menos de 20 chars, achamos ruim\n",
    "    # Se já revisou 3 vezes, aceitamos de qualquer jeito para evitar loop infinito\n",
    "    if len(state['resposta']) < 20 and state['revisoes'] < 3:\n",
    "        return {\"pergunta\": state['pergunta'] + \" (Seja mais detalhado!)\"}\n",
    "    return {} # Nenhuma mudança"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definindo a Lógica Condicional (Edges)\n",
    "\n",
    "Decide se volta para o gerador ou termina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:06.806599Z",
     "iopub.status.busy": "2026-02-02T02:36:06.806094Z",
     "iopub.status.idle": "2026-02-02T02:36:06.878164Z",
     "shell.execute_reply": "2026-02-02T02:36:06.877791Z"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def router(state: AgenteState):\n",
    "    pass # Script-patched: ensure non-empty block\n",
    "    # Se a resposta for curta e tivermos revisões sobrando, volta pro gerador\n",
    "    if len(state['resposta']) < 20 and state['revisoes'] < 3:\n",
    "        return \"gerador\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Montando o Grafo\n",
    "\n",
    "Adicionamos nós e arestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:06.880057Z",
     "iopub.status.busy": "2026-02-02T02:36:06.879915Z",
     "iopub.status.idle": "2026-02-02T02:36:06.884639Z",
     "shell.execute_reply": "2026-02-02T02:36:06.884163Z"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(AgenteState)\n",
    "\n",
    "workflow.add_node(\"gerador\", gerador)\n",
    "workflow.add_node(\"critico\", critico)\n",
    "\n",
    "# Fluxo: Start -> Gerador -> Critico -> Router -> (Gerador ou Fim)\n",
    "workflow.add_edge(START, \"gerador\")\n",
    "workflow.add_edge(\"gerador\", \"critico\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"critico\",\n",
    "    router,\n",
    "    {\n",
    "        \"gerador\": \"gerador\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Executando\n",
    "\n",
    "Vamos fazer uma pergunta que exige resposta curta para ver se ele entra no loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:36:06.886435Z",
     "iopub.status.busy": "2026-02-02T02:36:06.886196Z",
     "iopub.status.idle": "2026-02-02T02:36:07.646416Z",
     "shell.execute_reply": "2026-02-02T02:36:07.645276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GERANDO (Tentativa 0) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CRITICANDO ---\n",
      "\n",
      "=== FINAL ===\n",
      "Pedro Álvares Cabral.\n",
      "Total revisões: 1\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"pergunta\": \"Quem descobriu o Brasil?\", \"revisoes\": 0, \"resposta\": \"\"}\n",
    "output = app.invoke(inputs)\n",
    "\n",
    "print(\"\\n=== FINAL ===\")\n",
    "print(output['resposta'])\n",
    "print(f\"Total revisões: {output['revisoes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Criamos um sistema que se auto-corrige! O `LangGraph` permite criar fluxos de trabalho resilientes onde o agente pode tentar novamente se falhar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
