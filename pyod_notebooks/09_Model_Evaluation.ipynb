{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 9: Avaliação de Modelos e Definição de Baseline\n",
                "\n",
                "## Objetivo\n",
                "Aprender a avaliar se o seu modelo de detecção de anomalias \"funciona\" e como definir o ponto de corte (threshold).\n",
                "\n",
                "## O Desafio\n",
                "Em detecção de anomalias **não-supervisionada**, você não tem labels (não sabe o que é fraude). Como saber se o modelo é bom?\n",
                "1.  **Avaliação Interna**: Estabilidade, consenso entre modelos.\n",
                "2.  **Avaliação Externa (com Labels)**: Usando um pequeno conjunto anotado manualmente por auditores.\n",
                "\n",
                "Neste notebook, assumiremos que auditamos uma amostra piloto e temos os labels verdadeiros para validar o modelo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q pyod pandas matplotlib seaborn scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from pyod.models.iforest import IForest\n",
                "from pyod.utils.data import generate_data\n",
                "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
                "\n",
                "plt.rcParams['figure.figsize'] = (10, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Gerando Dados e Treinando Modelo\n",
                "Usaremos iForest como exemplo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = generate_data(n_train=1000, n_test=500, contamination=0.1, random_state=42)\n",
                "\n",
                "clf = IForest(random_state=42)\n",
                "clf.fit(X_train)\n",
                "\n",
                "# Obtendo scores de teste\n",
                "y_scores = clf.decision_function(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. A Curva ROC e AUC\n",
                "A métrica AUC (Area Under Curve) diz o quão bem o modelo separa as classes, INDEPENDENTE do threshold escolhido.\n",
                "- AUC = 0.5: Aleatório\n",
                "- AUC = 1.0: Perfeito"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
                "roc_auc = auc(fpr, tpr)\n",
                "\n",
                "plt.figure()\n",
                "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (area = {roc_auc:.2f})')\n",
                "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('Taxa de Falsos Positivos (FPR)')\n",
                "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
                "plt.title('Receiver Operating Characteristic (ROC)')\n",
                "plt.legend(loc=\"lower right\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Curva Precision-Recall\n",
                "Em detecção de fraude, os dados são muito desbalanceados (poucas fraudes). A curva ROC pode ser otimista demais. A curva Precision-Recall é mais honesta."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
                "avg_precision = average_precision_score(y_test, y_scores)\n",
                "\n",
                "plt.figure()\n",
                "plt.plot(recall, precision, color='green', lw=2, label=f'Precision-Recall (avg = {avg_precision:.2f})')\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "plt.title('Curva Precision-Recall')\n",
                "plt.legend(loc=\"lower left\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Precision @ k\n",
                "Esta é a métrica **mais importante para auditores**.\n",
                "Se o auditor só tem tempo para verificar 50 casos (`k=50`), quantos desses 50 são realmente fraudes?\n",
                "\n",
                "Isso mede a qualidade do ranking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyod.utils.utility import precision_n_scores\n",
                "\n",
                "# Calcula precisão @ n (onde n é o número de outliers reais existentes no dataset)\n",
                "p_at_n = precision_n_scores(y_test, y_scores)\n",
                "print(f\"Precision @ n: {p_at_n:.2f}\")\n",
                "\n",
                "# Exemplo manual: Precision @ Top 10\n",
                "df_res = pd.DataFrame({'score': y_scores, 'label': y_test})\n",
                "df_res = df_res.sort_values('score', ascending=False)\n",
                "\n",
                "top_10 = df_res.head(10)\n",
                "acertos_top_10 = top_10['label'].sum()\n",
                "print(f\"Em 10 casos investigados de maior score, encontramos {int(acertos_top_10)} fraudes reais.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusão\n",
                "Metrics como Precision@K são muito mais úteis para o negócio do que acurácia simples, pois refletem a eficiência da força de trabalho da auditoria."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}