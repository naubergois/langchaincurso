{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. RAG Parte 3: RetrievalQA\n",
    "\n",
    "Agora vamos juntar as peças: Docs -> Split -> Vector Store -> Retriever -> LLM -> Resposta.\n",
    "\n",
    "**Objetivos:**\n",
    "- Criar uma `create_retrieval_chain` para responder perguntas baseadas nos documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a5462",
   "metadata": {},
   "source": [
    "# Explicação Detalhada do Assunto\n",
    "\n",
    "# 07. RAG Parte 3: RetrievalQA\n",
    "\n",
    "Neste notebook, vamos consolidar o que aprendemos sobre Retrieval Augmented Generation (RAG) e construir um sistema completo e funcional. Uniremos as peças: documentos, divisão (splitting), armazenamento de vetores (vector store), recuperação (retriever), LLM (Large Language Model) e, finalmente, a resposta!\n",
    "\n",
    "**Resumo Executivo:**\n",
    "\n",
    "Este notebook marca o ponto culminante da nossa jornada de RAG. Implementaremos um sistema completo que recebe uma pergunta, busca informações relevantes em uma base de conhecimento, e utiliza um LLM para gerar uma resposta fundamentada. Aprenderemos a orquestrar todos os componentes do pipeline RAG, desde o carregamento e processamento dos documentos até a geração da resposta final.\n",
    "\n",
    "**Conceitos Chave:**\n",
    "\n",
    "*   **RAG (Retrieval Augmented Generation):** Uma técnica que combina a capacidade de recuperação de informações de um sistema de busca com a habilidade de geração de texto de um modelo de linguagem. Isso permite que o LLM responda perguntas com base em informações externas, em vez de depender apenas do seu conhecimento pré-existente.\n",
    "*   **Chains (Correntes):** No contexto do LangChain, Chains são sequências de chamadas a componentes, como LLMs, prompts e utilitários. Elas permitem criar fluxos de trabalho complexos e automatizados. Neste notebook, usaremos chains para orquestrar o processo de recuperação e geração.\n",
    "*   **Retriever (Recuperador):** Um componente responsável por buscar documentos relevantes em uma base de conhecimento, com base em uma consulta. Usaremos um retriever para encontrar os documentos mais relevantes para responder à pergunta do usuário.\n",
    "*   **Vector Store (Armazenamento de Vetores):** Uma base de dados especializada em armazenar representações vetoriais de documentos. Isso permite realizar buscas semânticas eficientes, encontrando documentos que são semanticamente similares à consulta, mesmo que não compartilhem palavras-chave exatas.\n",
    "\n",
    "**Objetivos de Aprendizado:**\n",
    "\n",
    "Ao completar este notebook, você será capaz de:\n",
    "\n",
    "*   Construir um pipeline RAG completo usando LangChain.\n",
    "*   Implementar a função `create_retrieval_chain` para responder a perguntas com base em documentos recuperados.\n",
    "*   Entender como os diferentes componentes do pipeline RAG interagem entre si.\n",
    "*   Inspecionar as fontes de informação utilizadas para gerar uma resposta.\n",
    "*   Adaptar e personalizar o pipeline RAG para diferentes casos de uso.\n",
    "\n",
    "**Importância no Ecossistema LangChain:**\n",
    "\n",
    "O RAG é uma das aplicações mais importantes e poderosas do LangChain. Ele permite construir sistemas que podem responder a perguntas complexas, gerar conteúdo personalizado e realizar tarefas de raciocínio, tudo com base em informações externas e atualizadas. Dominar o RAG é fundamental para construir aplicações de IA Generativa robustas e úteis.\n",
    "\n",
    "Vamos começar!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:08.819427Z",
     "iopub.status.busy": "2026-02-03T11:18:08.819348Z",
     "iopub.status.idle": "2026-02-03T11:18:08.832962Z",
     "shell.execute_reply": "2026-02-03T11:18:08.832705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INJECTION START ###\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Carrega .env do local ou de pastas comuns\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Autenticação automática do script\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:08.834003Z",
     "iopub.status.busy": "2026-02-03T11:18:08.833923Z",
     "iopub.status.idle": "2026-02-03T11:18:08.835863Z",
     "shell.execute_reply": "2026-02-03T11:18:08.835646Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    pass # Script-patched: using env var\n",
    "except:\n",
    "    pass # Script-patched: using env var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Rápido (Load, Split, Index)\n",
    "\n",
    "Recriando o índice para usar aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:08.836901Z",
     "iopub.status.busy": "2026-02-03T11:18:08.836834Z",
     "iopub.status.idle": "2026-02-03T11:18:14.034273Z",
     "shell.execute_reply": "2026-02-03T11:18:14.033988Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.6). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:47: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google.generativeai.caching import CachedContent  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. Load\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Index\n",
    "vectorstore = FAISS.from_documents(splits, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando a Chain de RAG\n",
    "\n",
    "Usaremos `create_stuff_documents_chain` (que insere os docs no prompt) e `create_retrieval_chain` (que gerencia a busca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:14.035868Z",
     "iopub.status.busy": "2026-02-03T11:18:14.035771Z",
     "iopub.status.idle": "2026-02-03T11:18:14.102392Z",
     "shell.execute_reply": "2026-02-03T11:18:14.102100Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "# Prompt do sistema que receberá o contexto\n",
    "system_prompt = (\n",
    "    \"Você é um assistente para tarefas de perguntas e respostas. \"\n",
    "    \"Use os seguintes pedaços de contexto recuperado para responder à pergunta. \"\n",
    "    \"Se você não souber a resposta, diga que não sabe. \"\n",
    "    \"Use no máximo três frases e mantenha a resposta concisa.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain que combina documentos no prompt\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Chain final que recupera docs e passa para a chain acima\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testando\n",
    "\n",
    "Vamos fazer uma pergunta sobre o artigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:14.103856Z",
     "iopub.status.busy": "2026-02-03T11:18:14.103722Z",
     "iopub.status.idle": "2026-02-03T11:18:16.465119Z",
     "shell.execute_reply": "2026-02-03T11:18:16.464790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A decomposição de tarefas envolve dividir tarefas complexas em etapas menores e mais simples. Isso pode ser feito usando técnicas como Chain of Thought (CoT) ou Tree of Thoughts, ou usando LLMs com prompts simples. Alternativamente, pode ser feito usando instruções específicas da tarefa ou com entradas humanas.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspecionando a Fonte\n",
    "\n",
    "Podemos ver quais documentos foram usados para gerar a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:16.466978Z",
     "iopub.status.busy": "2026-02-03T11:18:16.466827Z",
     "iopub.status.idle": "2026-02-03T11:18:16.468642Z",
     "shell.execute_reply": "2026-02-03T11:18:16.468369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 0: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what...\n",
      "Documento 1: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are...\n",
      "Documento 2: Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subg...\n",
      "Documento 3: (3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With ...\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"Documento {i}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Temos um sistema de RAG funcional! Ele recupera informação relevante e responde de forma fundamentada.\n",
    "\n",
    "No próximo notebook, vamos sair do padrão \"pergunta-resposta\" e entrar no mundo dos **Agentes**, que podem usar ferramentas para agir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
