{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Memória\n",
    "\n",
    "Os modelos de linguagem são \"stateless\" (sem estado), ou seja, eles não lembram da conversa passada por padrão. Para criar chatbots, precisamos gerenciar o histórico da conversa e passá-lo a cada nova interação. O LangChain facilita isso.\n",
    "\n",
    "**Objetivos:**\n",
    "- Entender como funciona a memória no LCEL.\n",
    "- Usar `RunnableWithMessageHistory` para gerenciar histórico automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb9c32",
   "metadata": {},
   "source": [
    "# Explicação Detalhada do Assunto\n",
    "\n",
    "# 03. Memória\n",
    "\n",
    "Bem-vindo(a) ao notebook sobre memória em LangChain! Este é um pilar fundamental para a construção de aplicações de IA generativa conversacionais, como chatbots, que precisam manter o contexto das interações para fornecer respostas relevantes e coerentes.\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Neste notebook, exploraremos o conceito de memória em modelos de linguagem e como implementá-la utilizando LangChain. Veremos o problema da \"statelessness\" (ausência de estado) dos LLMs e aprenderemos a adicionar histórico de conversas às nossas aplicações, permitindo que elas \"se lembrem\" do que foi dito anteriormente. Utilizaremos a abordagem moderna do LangChain, com `RunnableWithMessageHistory`, para gerenciar o histórico de forma eficiente.\n",
    "\n",
    "## Conceitos Chave\n",
    "\n",
    "Para um melhor aproveitamento deste material, é importante ter uma compreensão básica dos seguintes conceitos:\n",
    "\n",
    "*   **LLMs (Large Language Models):** Modelos de linguagem de grande escala, como o Gemini, que são a base da IA generativa.\n",
    "*   **Statelessness:** A característica de LLMs de não manterem estado, ou seja, cada interação é independente das anteriores.\n",
    "*   **Chains:** Sequências de operações (prompts, LLMs, parsers) que definem o fluxo de processamento em LangChain.\n",
    "*   **Memória:** A capacidade de um sistema de IA de reter informações sobre interações passadas e usá-las para influenciar interações futuras.\n",
    "*   **`RunnableWithMessageHistory`:** Uma classe do LangChain que facilita a adição de histórico de mensagens a uma chain.\n",
    "*   **`ChatMessageHistory`:** Uma classe para armazenar o histórico de mensagens em memória.\n",
    "*   **`MessagesPlaceholder`:** Um placeholder em um prompt que permite injetar o histórico de mensagens.\n",
    "\n",
    "## Objetivos de Aprendizado\n",
    "\n",
    "Ao concluir este notebook, você será capaz de:\n",
    "\n",
    "*   Compreender o problema da falta de memória em LLMs.\n",
    "*   Implementar a memória em suas aplicações LangChain utilizando `RunnableWithMessageHistory`.\n",
    "*   Gerenciar o histórico de conversas com `ChatMessageHistory`.\n",
    "*   Utilizar `MessagesPlaceholder` para injetar o histórico em seus prompts.\n",
    "*   Construir chatbots que \"se lembram\" de interações passadas, fornecendo respostas mais contextuais e relevantes.\n",
    "\n",
    "## Importância no Ecossistema LangChain\n",
    "\n",
    "A memória é um componente essencial para a criação de aplicações de IA generativa avançadas. Sem memória, os chatbots são incapazes de manter conversas significativas e personalizadas. Este notebook fornece o conhecimento e as ferramentas necessárias para superar essa limitação, permitindo que você crie aplicações mais inteligentes e interativas. Dominar este conceito é crucial para aproveitar ao máximo o poder do LangChain e construir soluções de IA generativa de ponta. Vamos começar!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:36.267212Z",
     "iopub.status.busy": "2026-02-02T02:34:36.266853Z",
     "iopub.status.idle": "2026-02-02T02:34:36.288768Z",
     "shell.execute_reply": "2026-02-02T02:34:36.288333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INJECTION START ###\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Carrega .env do local ou de pastas comuns\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Autenticação automática do script\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Look for .env in scripts folder\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# !pip install -qU langchain langchain-openai langchain-community python-dotenv # Script-patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:36.290821Z",
     "iopub.status.busy": "2026-02-02T02:34:36.290641Z",
     "iopub.status.idle": "2026-02-02T02:34:36.293548Z",
     "shell.execute_reply": "2026-02-02T02:34:36.293201Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    pass # Script-patched\n",
    "except:\n",
    "    pass # Script-patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:36.295039Z",
     "iopub.status.busy": "2026-02-02T02:34:36.294909Z",
     "iopub.status.idle": "2026-02-02T02:34:36.805729Z",
     "shell.execute_reply": "2026-02-02T02:34:36.805456Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.6). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:47: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google.generativeai.caching import CachedContent  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. O Problema da Falta de Memória\n",
    "\n",
    "Vamos ver como o modelo se comporta sem memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:36.807216Z",
     "iopub.status.busy": "2026-02-02T02:34:36.807080Z",
     "iopub.status.idle": "2026-02-02T02:34:38.999714Z",
     "shell.execute_reply": "2026-02-02T02:34:38.998913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá Nauber, tudo bem? É um prazer te conhecer! Em que posso te ajudar hoje?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu sou um modelo de linguagem grande, treinado pelo Google. Eu não tenho nome.\n"
     ]
    }
   ],
   "source": [
    "chain = ChatPromptTemplate.from_template(\"{input}\") | llm | StrOutputParser()\n",
    "\n",
    "# Primeira interação\n",
    "print(chain.invoke({\"input\": \"Oi, meu nome é Nauber.\"}))\n",
    "\n",
    "# Segunda interação\n",
    "print(chain.invoke({\"input\": \"Qual é o meu nome?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ele provavlemente dirá que não sabe, pois cada chamada é independente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adicionando Histórico com `RunnableWithMessageHistory`\n",
    "\n",
    "Essa é a forma recomendada no LCEL moderno. Precisamos de uma classe para armazenar o histórico (aqui usaremos `ChatMessageHistory` em memória, mas poderia ser num banco de dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:39.002390Z",
     "iopub.status.busy": "2026-02-02T02:34:39.002189Z",
     "iopub.status.idle": "2026-02-02T02:34:39.016228Z",
     "shell.execute_reply": "2026-02-02T02:34:39.015929Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Dicionário para guardar os históricos de diferentes sessões (session_ids)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora criamos o prompt aceitando um `MessagesPlaceholder` para injetar o histórico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:39.017541Z",
     "iopub.status.busy": "2026-02-02T02:34:39.017449Z",
     "iopub.status.idle": "2026-02-02T02:34:39.020621Z",
     "shell.execute_reply": "2026-02-02T02:34:39.020422Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt_with_history = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente prestativo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "runnable = prompt_with_history | llm | StrOutputParser()\n",
    "\n",
    "# Envolvemos a chain original com a capacidade de histórico\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testando a Memória\n",
    "\n",
    "Agora vamos conversar passando um `session_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:39.021670Z",
     "iopub.status.busy": "2026-02-02T02:34:39.021597Z",
     "iopub.status.idle": "2026-02-02T02:34:40.323527Z",
     "shell.execute_reply": "2026-02-02T02:34:40.323242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta 1: Olá Nauber, é um prazer conhecê-lo! Em que posso ajudá-lo hoje?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta 2: Seu nome é Nauber.\n"
     ]
    }
   ],
   "source": [
    "# Configurando o ID da sessão\n",
    "config = {\"configurable\": {\"session_id\": \"sessao_do_nauber\"}}\n",
    "\n",
    "response1 = with_message_history.invoke(\n",
    "    {\"input\": \"Oi, meu nome é Nauber.\"}, \n",
    "    config=config\n",
    ")\n",
    "print(f\"Resposta 1: {response1}\")\n",
    "\n",
    "response2 = with_message_history.invoke(\n",
    "    {\"input\": \"Qual é o meu nome?\"}, \n",
    "    config=config\n",
    ")\n",
    "print(f\"Resposta 2: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chats Diferentes (Session IDs)\n",
    "\n",
    "Se mudarmos o `session_id`, ele não lembrará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:34:40.324972Z",
     "iopub.status.busy": "2026-02-02T02:34:40.324886Z",
     "iopub.status.idle": "2026-02-02T02:34:41.031081Z",
     "shell.execute_reply": "2026-02-02T02:34:41.030776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta 3 (Sessão Nova): Eu não tenho acesso a informações pessoais sobre você, então eu não sei o seu nome.\n"
     ]
    }
   ],
   "source": [
    "config_novo = {\"configurable\": {\"session_id\": \"sessao_nova\"}}\n",
    "\n",
    "response3 = with_message_history.invoke(\n",
    "    {\"input\": \"Qual é o meu nome?\"}, \n",
    "    config=config_novo\n",
    ")\n",
    "print(f\"Resposta 3 (Sessão Nova): {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, aprendemos a manter o estado da conversa usando `RunnableWithMessageHistory` e `ChatMessageHistory`.\n",
    "\n",
    "No próximo notebook, vamos explorar **Chains** mais complexas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
