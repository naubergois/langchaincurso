{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Projeto Final: Chatbot com RAG\n",
    "\n",
    "Chegamos ao final! Vamos consolidar tudo o que aprendemos criando um Chatbot estilo \"ChatPDF\". O usuário fará upload de um PDF e poderá conversar sobre ele.\n",
    "\n",
    "**Componentes:**\n",
    "- Upload de arquivo.\n",
    "- PyPDFLoader.\n",
    "- Text Splitting & Embeddings.\n",
    "- Retrieval Chain.\n",
    "- Loop de Chat interativo.\n",
    "- Memória de Conversa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf55eb",
   "metadata": {},
   "source": [
    "# Explicação Detalhada do Assunto\n",
    "\n",
    "# 10. Projeto Final: Chatbot com RAG\n",
    "\n",
    "Bem-vindo ao décimo e último notebook desta série! Aqui, vamos consolidar todo o conhecimento que você adquiriu ao longo do curso criando um chatbot completo, similar ao \"ChatPDF\". Prepare-se para construir uma aplicação interativa que permite aos usuários fazer upload de um arquivo PDF e conversar com ele, obtendo respostas precisas e relevantes.\n",
    "\n",
    "## Resumo Executivo\n",
    "\n",
    "Este notebook é o culminar de tudo o que aprendemos sobre LangChain e IA Generativa. Vamos desenvolver um chatbot que utiliza a técnica de Recuperação Aumentada de Geração (RAG) para responder a perguntas sobre um documento PDF fornecido pelo usuário. Este projeto prático demonstra a aplicação real de conceitos como loaders, splitters, embeddings, vector stores, chains e memória.\n",
    "\n",
    "## Conceitos Chave\n",
    "\n",
    "Para construir nosso chatbot, utilizaremos os seguintes conceitos:\n",
    "\n",
    "*   **Chains:** Sequências de operações que combinam diferentes componentes do LangChain para realizar uma tarefa específica, como processar uma pergunta e gerar uma resposta.\n",
    "*   **RAG (Retrieval-Augmented Generation):** Uma técnica que combina a capacidade de recuperação de informações relevantes de um documento (Retrieval) com a capacidade de gerar texto coerente e informativo (Generation). Isso permite que o chatbot responda a perguntas com base no conteúdo do PDF, em vez de apenas em seu próprio conhecimento pré-existente.\n",
    "*   **Memória:** A capacidade do chatbot de lembrar conversas anteriores, permitindo que ele entenda o contexto e responda de forma mais coerente e relevante. Usaremos `create_history_aware_retriever` para reformular a pergunta com base no histórico.\n",
    "*   **Loaders:** Componentes que carregam dados de diversas fontes, como arquivos PDF.\n",
    "*   **Splitters:** Componentes que dividem documentos grandes em partes menores para facilitar o processamento.\n",
    "*   **Embeddings:** Representações numéricas de texto que capturam o significado semântico das palavras e frases.\n",
    "*   **Vector Stores:** Bancos de dados que armazenam embeddings para permitir a busca eficiente de informações relevantes.\n",
    "\n",
    "## Objetivos de Aprendizado\n",
    "\n",
    "Ao completar este notebook, você será capaz de:\n",
    "\n",
    "*   Construir um chatbot funcional que utiliza a técnica de RAG.\n",
    "*   Implementar a funcionalidade de upload de arquivos PDF.\n",
    "*   Processar documentos PDF, dividindo-os em partes menores e gerando embeddings.\n",
    "*   Armazenar embeddings em um vector store.\n",
    "*   Criar chains que combinam a recuperação de informações com a geração de respostas.\n",
    "*   Implementar a memória para manter o contexto da conversa.\n",
    "*   Interagir com o chatbot e obter respostas relevantes sobre o conteúdo do PDF.\n",
    "\n",
    "## Importância no Ecossistema LangChain\n",
    "\n",
    "Este projeto demonstra a aplicação prática de muitos dos conceitos fundamentais do LangChain. A capacidade de construir chatbots que podem conversar sobre documentos específicos é extremamente valiosa em diversas áreas, como suporte ao cliente, educação, pesquisa e muito mais. Dominar essas técnicas o colocará em uma posição vantajosa para desenvolver soluções de IA Generativa inovadoras e eficazes.\n",
    "\n",
    "Vamos começar!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:25.292151Z",
     "iopub.status.busy": "2026-02-03T11:18:25.292080Z",
     "iopub.status.idle": "2026-02-03T11:18:25.300309Z",
     "shell.execute_reply": "2026-02-03T11:18:25.300059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INJECTION START ###\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Carrega .env do local ou de pastas comuns\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Autenticação automática do script\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# !pip install -qU langchain langchain-openai langchain-community faiss-cpu pypdf python-dotenv # Script-patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:25.301376Z",
     "iopub.status.busy": "2026-02-03T11:18:25.301303Z",
     "iopub.status.idle": "2026-02-03T11:18:25.303319Z",
     "shell.execute_reply": "2026-02-03T11:18:25.303102Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "except ImportError:\n",
    "    userdata = None\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    pass # Script-patched: using env var\n",
    "except:\n",
    "    pass # Script-patched: using env var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload do PDF\n",
    "\n",
    "Usaremos a funcionalidade do Colab para upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:25.304407Z",
     "iopub.status.busy": "2026-02-03T11:18:25.304338Z",
     "iopub.status.idle": "2026-02-03T11:18:25.306281Z",
     "shell.execute_reply": "2026-02-03T11:18:25.306078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Colab not detected. Using local file.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    print(\"Faça upload do seu PDF:\")\n",
    "    uploaded = files.upload()\n",
    "    filename = next(iter(uploaded))\n",
    "    print(f\"Arquivo {filename} carregado.\")\n",
    "except ImportError:\n",
    "    print(\"Google Colab not detected. Using local file.\")\n",
    "    # Fallback for local execution\n",
    "#     filename = os.getenv(\"GOOGLE_API_KEY\") # Script-patched: using env var from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processamento (ETL)\n",
    "\n",
    "Loader -> Splitter -> VectorStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:25.307408Z",
     "iopub.status.busy": "2026-02-03T11:18:25.307336Z",
     "iopub.status.idle": "2026-02-03T11:18:27.240286Z",
     "shell.execute_reply": "2026-02-03T11:18:27.240010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.6). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/auth/__init__.py:54: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/google/oauth2/__init__.py:40: FutureWarning: You are using a Python version 3.9 past its end of life. Google will update google-auth with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade your Python version, and then update google-auth.\n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/langchain_google_genai/chat_models.py:47: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google.generativeai.caching import CachedContent  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Loader\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index\n",
    "vectorstore = FAISS.from_documents(splits, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurando a Chain com Histórico\n",
    "\n",
    "Vamos usar `create_history_aware_retriever` para reformular a pergunta com base no histórico antes de buscar, garantindo que o chat flua bem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:27.241773Z",
     "iopub.status.busy": "2026-02-03T11:18:27.241634Z",
     "iopub.status.idle": "2026-02-03T11:18:27.291571Z",
     "shell.execute_reply": "2026-02-03T11:18:27.291310Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "### 1. Contextualize Question ###\n",
    "# Reformula a pergunta para incluir o contexto do histórico\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Dada uma conversa e a última pergunta do usuário, \"\n",
    "    \"se ela referir-se ao contexto anterior, formule uma nova pergunta autônoma \"\n",
    "    \"que seja compreensível sem o histórico. Não responda à pergunta, \"\n",
    "    \"apenas reformule-a se necessário, caso contrário retorne-a como está.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### 2. Answer Question ###\n",
    "# Responde à pergunta usando os docs recuperados\n",
    "qa_system_prompt = (\n",
    "    \"Você é um assistente útil para tarefas de perguntas e respostas. \"\n",
    "    \"Use os seguintes pedaços de contexto recuperado para responder à pergunta. \"\n",
    "    \"Se você não souber a resposta, diga que não sabe. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "### 3. State Management ###\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat Loop\n",
    "\n",
    "Vamos conversar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:18:27.292914Z",
     "iopub.status.busy": "2026-02-03T11:18:27.292776Z",
     "iopub.status.idle": "2026-02-03T11:18:27.295023Z",
     "shell.execute_reply": "2026-02-03T11:18:27.294817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Olá! Pergunte qualquer coisa sobre o documento. Digite 'sair' para encerrar.\n",
      "Bot: Até mais!\n"
     ]
    }
   ],
   "source": [
    "session_id = \"minha_sessao\"\n",
    "\n",
    "print(\"Bot: Olá! Pergunte qualquer coisa sobre o documento. Digite 'sair' para encerrar.\")\n",
    "\n",
    "while True:\n",
    "    pass # Script-patched: ensure non-empty block\n",
    "#     user_input = os.getenv(\"GOOGLE_API_KEY\") # Script-patched: using env var from .env\n",
    "    user_input = \"sair\"\n",
    "    if user_input.lower() in [\"sair\", \"quit\", \"exit\"]:\n",
    "        print(\"Bot: Até mais!\")\n",
    "        break\n",
    "    \n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    \n",
    "    print(f\"Bot: {response['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão do Curso\n",
    "\n",
    "Parabéns! Você completou os 10 exemplos práticos de LangChain.\n",
    "\n",
    "**Resumo do que aprendemos:**\n",
    "1. Chamar Modelos (LLMs e ChatModels).\n",
    "2. Usar Prompt Templates e Output Parsers.\n",
    "3. Adicionar Memória.\n",
    "4. Criar Chains Sequenciais e Paralelas.\n",
    "5. Carregar Documentos (ETL).\n",
    "6. Criar Embeddings e Vector Stores.\n",
    "7. Construir sistemas de RAG.\n",
    "8. Usar Agentes com Ferramentas prontas.\n",
    "9. Criar Ferramentas Customizadas.\n",
    "10. Integrar tudo em uma aplicação completa.\n",
    "\n",
    "Continue explorando a documentação oficial para recursos mais avançados como LangGraph e LangServe!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
