{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. RAG Parte 1: Document Loaders e Text Splitters\n",
    "\n",
    "Para que o LLM responda sobre dados que ele não conhece (dados privados), usamos RAG (Retrieval Augmented Generation). O primeiro passo é o **ETL** (Extract, Transform, Load): extrair o texto de fontes e dividi-lo em pedaços menores (chunks).\n",
    "\n",
    "**Objetivos:**\n",
    "- Carregar dados de uma URL (`WebBaseLoader`).\n",
    "- Dividir o texto em partes gerenciáveis (`RecursiveCharacterTextSplitter`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767f53d",
   "metadata": {},
   "source": [
    "# Explicação Detalhada do Assunto\n",
    "\n",
    "# 05. RAG Parte 1: Document Loaders e Text Splitters\n",
    "\n",
    "Bem-vindo ao primeiro passo na construção de aplicações poderosas com RAG (Retrieval Augmented Generation)! Neste notebook, vamos mergulhar no processo fundamental de preparação dos seus dados para que um LLM (Large Language Model) possa acessá-los e utilizá-los para gerar respostas informativas e relevantes.\n",
    "\n",
    "**Resumo Executivo:**\n",
    "\n",
    "Este notebook é dedicado ao processo de ETL (Extract, Transform, Load) aplicado a documentos para uso em sistemas RAG. Abordaremos como carregar documentos de diversas fontes usando LangChain Document Loaders e como dividir esses documentos em chunks menores e mais gerenciáveis usando Text Splitters.\n",
    "\n",
    "**Conceitos Chave:**\n",
    "\n",
    "*   **RAG (Retrieval Augmented Generation):** Uma técnica que combina a capacidade de um LLM de gerar texto com a capacidade de buscar informações relevantes de uma base de dados externa. Isso permite que o LLM responda a perguntas sobre dados que não fazem parte de seu conhecimento prévio.\n",
    "*   **ETL (Extract, Transform, Load):** Um processo padrão em ciência de dados que envolve extrair dados de uma ou mais fontes, transformar esses dados para torná-los utilizáveis e, finalmente, carregar os dados transformados em um sistema de destino. No contexto de RAG, o ETL se refere à preparação dos documentos para serem indexados e utilizados pelo LLM.\n",
    "*   **Document Loaders:** Ferramentas do LangChain que permitem carregar documentos de diversas fontes, como PDFs, websites, arquivos CSV, etc.\n",
    "*   **Text Splitters:** Ferramentas do LangChain que dividem um texto longo em pedaços menores (chunks) para facilitar a indexação e a busca de informações relevantes. A divisão em chunks ajuda a otimizar o uso de tokens e a melhorar a precisão da busca.\n",
    "\n",
    "**Objetivos de Aprendizado:**\n",
    "\n",
    "Ao concluir este notebook, você será capaz de:\n",
    "\n",
    "*   Utilizar `Document Loaders` do LangChain para carregar dados de diferentes fontes (ex: web pages).\n",
    "*   Empregar `Text Splitters` do LangChain para dividir documentos longos em chunks menores e mais adequados para RAG.\n",
    "*   Compreender a importância do tamanho do chunk e do overlap para um bom funcionamento do RAG.\n",
    "*   Preparar seus documentos para o próximo passo do processo RAG: a criação de embeddings e a indexação em um vector store.\n",
    "\n",
    "**Importância no Ecossistema LangChain:**\n",
    "\n",
    "A etapa de carregamento e divisão de documentos é absolutamente crucial para o sucesso de qualquer aplicação RAG. Sem dados bem preparados, o LLM não conseguirá acessar as informações relevantes e gerar respostas precisas. Dominar o uso de Document Loaders e Text Splitters é, portanto, um passo fundamental para se tornar um especialista em LangChain e IA Generativa.\n",
    "\n",
    "Vamos começar a construir juntos!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:17:16.163753Z",
     "iopub.status.busy": "2026-02-03T11:17:16.163659Z",
     "iopub.status.idle": "2026-02-03T11:17:18.583546Z",
     "shell.execute_reply": "2026-02-03T11:17:18.583108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INJECTION START ###\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Carrega .env do local ou de pastas comuns\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "### INJECTION END ###\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Autenticação automática do script\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "# Look for .env in scripts folder\n",
    "for p in ['.', '..', 'scripts', '../scripts']:\n",
    "    path = os.path.join(p, '.env')\n",
    "    if os.path.exists(path):\n",
    "        load_dotenv(path)\n",
    "        break\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# !pip install -qU langchain langchain-community beautifulsoup4 # Script-patched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregando Documentos (Loaders)\n",
    "\n",
    "Existem loaders para PDF, CSV, TXT, Notion, Youtube, etc. Vamos usar o `WebBaseLoader` para pegar o conteúdo de uma página web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:17:18.585792Z",
     "iopub.status.busy": "2026-02-03T11:17:18.585683Z",
     "iopub.status.idle": "2026-02-03T11:17:23.238536Z",
     "shell.execute_reply": "2026-02-03T11:17:23.238024Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naubergois/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de documentos carregados: 1\n",
      "Tamanho do conteúdo: 10992 caracteres\n",
      "Quickstart - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedQuickstartLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineering\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Vamos carregar um artigo do blog do LangChain, por exemplo\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/introduction\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Número de documentos carregados: {len(docs)}\")\n",
    "print(f\"Tamanho do conteúdo: {len(docs[0].page_content)} caracteres\")\n",
    "print(docs[0].page_content[:500]) # Primeiros 500 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividindo o Texto (Splitters)\n",
    "\n",
    "Modelos têm limite de tokens. Além disso, para buscar informação, é melhor ter pedaços pequenos e específicos do que um texto gigante. O `RecursiveCharacterTextSplitter` é o mais comum, pois tenta quebrar em parágrafos e frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:17:23.240517Z",
     "iopub.status.busy": "2026-02-03T11:17:23.240387Z",
     "iopub.status.idle": "2026-02-03T11:17:23.243584Z",
     "shell.execute_reply": "2026-02-03T11:17:23.243173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram criados 15 pedaços (chunks).\n",
      "Conteúdo do primeiro chunk:\n",
      "Quickstart - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedQuickstartLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageRequirementsBuild a basic agentBuild a real-world agentGet startedQuickstartCopy pageCopy pageThis quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Tamanho alvo de cada pedaço\n",
    "    chunk_overlap=200     # Sobreposição para manter contexto nas bordas\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Foram criados {len(splits)} pedaços (chunks).\")\n",
    "print(f\"Conteúdo do primeiro chunk:\\n{splits[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Agora temos nossos documentos processados e prontos para serem indexados.\n",
    "\n",
    "No próximo notebook, vamos transformar esses chunks em vetores (Embeddings) e salvá-los em um Vector Store."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
